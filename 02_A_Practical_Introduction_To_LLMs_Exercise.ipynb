{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mM6vusQZx2N"
   },
   "source": [
    "# Choosing The Right LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ1BLXwtkyCV"
   },
   "source": [
    "## Activity ðŸŒŸ\n",
    "\n",
    "BrightText, a company offering text summarisation services, seeks an automated solution to provide concise, accurate, and scalable summaries of large volumes of text, such as reports, articles, and research papers. The solution must operate within limited computational resources, ensuring efficiency for deployment on edge devices or lightweight servers. Given the need to handle variable input lengths and complex content, the model must deliver consistent, high-quality results while maintaining low operational costs. Additionally, the solution should be adaptable to multiple languages and specialized domains, ensuring accurate, bias-free summaries that meet client-specific requirements without compromising on speed or reliability.\n",
    "\n",
    "Based on the above problem statement:\n",
    "* Go to the Hugging Face model hub and search for appropriate models for your problem\n",
    "* Pick **two** which you want to experiment with\n",
    "* Use `AutoModelForCausalLM` and `AutoTokenizer` to download and instantiate the models\n",
    "* Create a `pipeline` object\n",
    "* Use the `pipeline` object to summarise the file `article.txt` in the sample_data folder\n",
    "* Which model do you think performed better?\n",
    "\n",
    "You will want to drop the article.txt file into the sample_data directory in colab to make it easy to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFPMp6VnO1OA"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4nZNk5qVNw7N"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPQJKRwfF79"
   },
   "source": [
    "## ðŸ”§ Step 1: Install Required Packages\n",
    "We install `transformers` and `accelerate` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFw8-T4OxtKd",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## ðŸ¤– Step 2: Import Required Libraries\n",
    "We import:\n",
    "* AutoModelForCausalLM\n",
    "* AutoTokenizer\n",
    "* pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEnaReNLOOXB",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9"
   },
   "source": [
    "## ðŸ”„ Step 3: Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtYr8KlHOPpR",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## ðŸ§  Step 4: Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qat4NlZx6Uw",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## ðŸ—ž Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJoYGx66ymUn"
   },
   "source": [
    "We create the `pipeline` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qYQwbObyS0G",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnhqZpXv1eur"
   },
   "source": [
    "## ðŸ§ª Step 6: Test the model by instructing (prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF7sGjc0ydQ7"
   },
   "source": [
    "Read the file `article.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTu16ydIy-Wi",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsZMfPi88IYK"
   },
   "source": [
    "Pass the summarisation prompt to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COQS4fKt8WvD",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8ZSP-Ra9xPT"
   },
   "source": [
    "Replicating the same workflow for the other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F6A7; **You will need to restart the notebook kernel before starting the exercise otherwise you will run out of GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xvps7UpznPD4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vbYhZiVb05j"
   },
   "source": [
    "## Stretch activities ðŸŒŸ\n",
    "\n",
    "You've chosen LLMs that fit a certain brief. Now consolidate you knowledge with some strech activities:\n",
    "\n",
    "1. Think how you could evaluate the model summary that was generated above beyond just taking a look at it.\n",
    "2. Build the RAG system into a multi agent swarm.\n",
    "3. Figure out a better way of evaluating the response of LLM with respect to the ground truth."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
