{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhf5MqnS9oG"
   },
   "source": [
    "# Demo: Text Generation With LLMs\n",
    "In this notebook, weâ€™ll use Zephyr-7b beta (a fine tuned version of Mistral-7B, developed by mistral.ai), to create a short, engaging story for a two-year-old. With its advanced language capabilities, Zephyr-7B can help you to craft simple, friendly tales perfect for young listeners.\n",
    "\n",
    "Letâ€™s dive in and bring a fun story to life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC5KmEpIyr0a"
   },
   "source": [
    "Below, we filter out any annoying warning messages that might pop up when using certain libraries. Your code will run the same without doing this, we just prefer a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgcmH5bSM-x8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlSyUjRPRoqy"
   },
   "source": [
    "## ðŸ”§ Step 1: Install Required Packages\n",
    "\n",
    "We install the HuggingFace `transformers` package which contains lots of useful functionality when building transformer models.\n",
    "\n",
    "We also install `accelerate` to let us avoid writing boilerplate code needed to use multi-GPUs/TPU/fp16.\n",
    "\n",
    "*Note: it's atypical to install packages one at a time, line-by-line, but sometimes Sagemaker notebooks don't pick up all the install instructions when done correctly. We use %%capture to hide the console output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeJ8jRVzygmE"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UV7KvIT1O8G"
   },
   "source": [
    "## ðŸ¤– Step 2: Import Required Libraries\n",
    "We import:\n",
    "* AutoModelForCausalLM\n",
    "* AutoTokenizer\n",
    "* pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9RnBxBC2TCb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "305pOtCGDCc9"
   },
   "source": [
    "## ðŸ”„ Step 3: Load the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97qKynwwzbPm"
   },
   "source": [
    "Using the from_pretrained function, we download the tokenizer used by Zephyr-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf-Jrl4MzBkh"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfbMlQAk9Vf1"
   },
   "source": [
    "## ðŸ§  Step 4: Load the model - Zephyr-7B\n",
    "\n",
    "### Architectural details\n",
    "Zephyr-7B is a decoder-only Transformer with the following architectural choices:\n",
    "\n",
    "* Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens\n",
    "* GQA (Grouped Query Attention) - allowing faster inference and lower cache size.\n",
    "* Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9w4oQT1-AC-"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_olWhO1TRG"
   },
   "source": [
    "## ðŸ—ž Step 5: Wrap everything in a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw6iC5JKzm_z"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500 # specify we only want 500 tokens back\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnhqZpXv1eur"
   },
   "source": [
    "## ðŸ§ª Step 6: Test the model by instructing (prompting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpEndPRP0-DX"
   },
   "outputs": [],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short story for a 2 year old\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfFhVFWRc7IZ"
   },
   "source": [
    "## Extension  ðŸŒŸ\n",
    "\n",
    "1. Change your prompt to generate a different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9j7gAN0FNGi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
